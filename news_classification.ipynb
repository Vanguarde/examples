{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T11:06:30.374884Z",
     "start_time": "2021-09-21T11:06:30.370342Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import datedelta\n",
    "from tqdm.notebook import tqdm\n",
    "from pymystem3 import Mystem\n",
    "import nltk\n",
    "from nltk import word_tokenize, ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import re\n",
    "import pickle\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "import guidedlda\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy import stats\n",
    "import xgboost\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T11:06:52.097318Z",
     "start_time": "2021-09-21T11:06:52.080851Z"
    }
   },
   "source": [
    "# Guided LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T11:08:53.773568Z",
     "start_time": "2021-09-21T11:08:53.761599Z"
    }
   },
   "source": [
    "Здесь я использую заранее отобранный сэмпл (сбалансированный по СМИ) и привожу его к виду для LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = pickle.load(open('model/text_tokens_100k.pkl', 'rb'))\n",
    "# подгружаю сформированный ранее словарь стоп-слов ()\n",
    "stop_words = pd.read_json('stop_words_russian.json', encoding='utf-8')[0].tolist()\n",
    "stop_words = ' | '.join(stop_words)\n",
    "trash_phrases = pickle.load(open('model/trash_phrases.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = Mystem()\n",
    "news_lem, acc_all = [], []\n",
    "#Обрабатываю по 1к, т.к. по одному слишком медленно https://habr.com/ru/post/503420/\n",
    "for i in tqdm(range(0, len(text_tokens), 1000)):\n",
    "    news_batch = news[i:i+1000]\n",
    "    text = ' div '.join(news_batch) # объединяю в одну строку\n",
    "    text = stem.lemmatize(text) # лемматизирую\n",
    "    text = ''.join(text) # объединяю токены назад (т.к. нужно разделить по 'div')\n",
    "    text = re.sub(stop_words, \" \", text) # убираю стоп-слова\n",
    "    text = re.sub(trash_phrases, \" \", text) # убираю мусор\n",
    "    text = re.sub('[^A-Za-zА-Яа-я\\s]', '', text) # убираю все символы, кроме букв, цифр, знаков препинания и пробелов\n",
    "    text = text.split('div') # разделяю обратно по div\n",
    "    news_lem.extend(text)\n",
    "# привожу к виду matrix of token counts для последующего использования в алгоритме (для LDA tf-idf и иные не подходят, \n",
    "# более сложные занимают больше времени, в условиях ограниченнного времени это оптимальный выбор)\n",
    "X = vectorizer.fit_transform(news_lem).astype(np.int)\n",
    "pickle.dump(vectorizer, open('model/vectorizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T11:19:09.451239Z",
     "start_time": "2021-09-21T11:19:09.433246Z"
    }
   },
   "source": [
    "Так как изначально буду использовать semi-supervised Guided (labeled) LDA, то необходимо подгрузить заранее сформированный мною словарь экономических слов (был сделан на основе словаря экономических новостей базы, экспертно взяты из него наиболее частые экономические термы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecwords = pd.read_excel('econom_dict.xlsx')['1gram'].tolist()\n",
    "# убираем из слова экономических термов те, которые не встретились в основном словаре\n",
    "ecwords = [x for x in ecwords if x in list(word2id.keys())]\n",
    "\n",
    "seed_topics = {}\n",
    "for word in ecwords:\n",
    "    seed_topics[word2id[word]] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T11:41:24.938396Z",
     "start_time": "2021-09-21T11:41:24.926385Z"
    }
   },
   "source": [
    "Строю Guided LDA модели от 2 до 14 тем, когерентность не смотрю, т.к. есть размеченный датасет для supervised learning, на котором я и проверю"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(2, 15)):\n",
    "    model = guidedlda.GuidedLDA(n_topics=i, n_iter=300, random_state=7, refresh=20,alpha=0.01,eta=0.01)\n",
    "    model.fit(X, seed_topics=seed_topics, seed_confidence=0.15)\n",
    "    pickle.dump(model, open(f'model/model_{i}.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T11:42:20.700854Z",
     "start_time": "2021-09-21T11:42:20.690350Z"
    }
   },
   "source": [
    "# Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T11:45:41.203250Z",
     "start_time": "2021-09-21T11:45:41.191478Z"
    }
   },
   "source": [
    "Заранее была проведена работа по разметке данных по ключевым словам. Были выделены 6 тем:  \n",
    "+ 0 - экономика  \n",
    "+ 1 - политика, в мире  \n",
    "+ 2 - общество  \n",
    "+ 3 - культура  \n",
    "+ 4 - проишествия  \n",
    "+ 5 - спорт  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T11:49:49.051199Z",
     "start_time": "2021-09-21T11:49:49.034733Z"
    }
   },
   "source": [
    "Датасет формировался следующим образом: 20к по каждой из тем. Далее был использован tf-idf для feature extraction, минимальное количество встречи слова - 100, максимальная частота - 0,8 (встречается менее, чем в 80% документов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T11:51:22.185811Z",
     "start_time": "2021-09-21T11:51:22.179290Z"
    }
   },
   "source": [
    "X был обработан аналогично, как для Guided LDA (стеммизирован+очищен от мусора/лишних знаков и т.д.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T11:51:42.503625Z",
     "start_time": "2021-09-21T11:51:42.491521Z"
    }
   },
   "source": [
    "y - список меток классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pickle.load(open('model_supervised/tfidf.pkl', 'rb'))\n",
    "dataset = pickle.load(open('markups keys/data/dataset.pkl', 'rb'))\n",
    "X = tfidf.transform(dataset)\n",
    "y = pickle.load(open('markups keys/data/y.pkl', 'rb'))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T12:50:33.141350Z",
     "start_time": "2021-09-21T12:50:33.040620Z"
    }
   },
   "source": [
    "Далее идем по списку supervised моделей и сравниваем по f1-srore, precision, recall. Плюсом проводим guided LDA через валидационную часть для получения сравнимых прогнозных мощностей моделей "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T12:53:15.870676Z",
     "start_time": "2021-09-21T12:53:15.855717Z"
    }
   },
   "source": [
    "Каждую модель провожу через cv, 5 folds с перемешиванием. CV по метрике f1-macro, т.к. выборка сбалансированная, то можно использовать данную метрику"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_5 = KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T12:48:44.093094Z",
     "start_time": "2021-09-21T12:48:44.085116Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T13:04:51.972527Z",
     "start_time": "2021-09-21T13:04:51.958057Z"
    },
    "hidden": true
   },
   "source": [
    "поиск по сетке идет на основе используемого регуляризатора (l1, l2 или без), константы, а также"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "grid = {'penalty': ['l1', 'l2', None],\n",
    "        \"C\":np.logspace(-4,4,20),\n",
    "        'fit_intercept': [True, False]}\n",
    "\n",
    "lr = LogisticRegression(n_jobs=4)\n",
    "clf_lr = RandomizedSearchCV(lr, \n",
    "                            param_distributions=grid,\n",
    "                            cv=kfold_5,  \n",
    "                            n_iter=5,\n",
    "                            scoring='f1_macro', \n",
    "                            error_score=0, \n",
    "                            verbose=3, \n",
    "                            n_jobs=-1)\n",
    "clf_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred_lr = clf_lr.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred_lr.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T13:05:03.223357Z",
     "start_time": "2021-09-21T13:05:03.210351Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "grid = {'kernel': ['linear'],\n",
    "        'gamma': [1e-3, 1e-4],\n",
    "        'C': [1, 100, 1000]}\n",
    "\n",
    "model_svm = SVC(random_state=42)\n",
    "clf_svm = RandomizedSearchCV(model_svm, \n",
    "                             param_distributions = grid,\n",
    "                             cv=kfold_5,  \n",
    "                             n_iter=5,\n",
    "                             scoring='f1_macro', \n",
    "                             error_score=0, \n",
    "                             verbose=3, \n",
    "                             n_jobs=-1)\n",
    "clf_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred_svm = clf_svm.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred_svm.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T13:07:17.272707Z",
     "start_time": "2021-09-21T13:07:17.256240Z"
    }
   },
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {'n_estimators': stats.randint(50, 2000),\n",
    "              'max_depth': stats.randint(10, 110),\n",
    "              'max_features': ['auto', 'sqrt'],\n",
    "              'min_samples_split': [2, 5, 10],\n",
    "              'min_samples_leaf': [1, 2, 4],\n",
    "              'bootstrap': [True, False]\n",
    "             }\n",
    "numFolds = 5\n",
    "kfold_5 = KFold(n_splits = numFolds, shuffle = True)\n",
    "\n",
    "clf = RandomizedSearchCV(rf, \n",
    "                         param_distributions = param_dist,\n",
    "                         cv = kfold_5,  \n",
    "                         n_iter = 5,\n",
    "                         scoring = 'f1_macro', \n",
    "                         error_score = 0, \n",
    "                         verbose = 3, \n",
    "                         n_jobs = -1)\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T13:15:19.838597Z",
     "start_time": "2021-09-21T13:15:19.832613Z"
    }
   },
   "source": [
    "## xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgboost.XGBClassifier(nthread=4, objective='binary:logistic')\n",
    "param_dist = {'n_estimators': stats.randint(20, 500),\n",
    "              \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "              'learning_rate': stats.uniform(0.01, 0.6),\n",
    "              'subsample': stats.uniform(0.3, 0.8),\n",
    "              'max_depth': [3, 4, 5, 6, 7, 8, 9],\n",
    "              'colsample_bytree': stats.uniform(0.5, 0.9),\n",
    "              'min_child_weight': [1, 2, 3, 4],\n",
    "              \"gamma\": [0, 0.1, 0.3,0.4],\n",
    "             }\n",
    "numFolds = 5\n",
    "kfold_5 = KFold(n_splits = numFolds, shuffle = True)\n",
    "\n",
    "clf = RandomizedSearchCV(model, \n",
    "                         param_distributions = param_dist,\n",
    "                         cv = kfold_5,  \n",
    "                         n_iter = 5,\n",
    "                         scoring = 'f1_macro', \n",
    "                         error_score = 0, \n",
    "                         verbose = 3, \n",
    "                         n_jobs = -1)\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T13:39:22.759952Z",
     "start_time": "2021-09-21T13:39:22.742956Z"
    }
   },
   "source": [
    "## xgboost 2-class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T13:40:16.456225Z",
     "start_time": "2021-09-21T13:40:16.437233Z"
    }
   },
   "source": [
    "Также для сравнения эффективности использования модель с большим количеством классов для лучшей разделимости тем была обучена 2-классовая модель xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ec = [0 if e==0 else 1 for e in y_train]\n",
    "y_test_ec = [0 if e==0 else 1 for e in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ec = xgboost.XGBClassifier(nthread=4, objective='binary:logistic')\n",
    "param_dist = {'n_estimators': stats.randint(20, 500),\n",
    "              \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "              'learning_rate': stats.uniform(0.01, 0.6),\n",
    "              'subsample': stats.uniform(0.3, 0.8),\n",
    "              'max_depth': [3, 4, 5, 6, 7, 8, 9],\n",
    "              'colsample_bytree': stats.uniform(0.5, 0.9),\n",
    "              'min_child_weight': [1, 2, 3, 4],\n",
    "              \"gamma\": [0, 0.1, 0.3,0.4],\n",
    "             }\n",
    "numFolds = 5\n",
    "kfold_5 = KFold(n_splits = numFolds, shuffle = True)\n",
    "\n",
    "clf_ec = RandomizedSearchCV(model_ec, \n",
    "                         param_distributions = param_dist,\n",
    "                         cv = kfold_5,  \n",
    "                         n_iter = 5,\n",
    "                         scoring = 'f1_macro', \n",
    "                         error_score = 0, \n",
    "                         verbose = 3, \n",
    "                         n_jobs = -1)\n",
    "\n",
    "clf_ec.fit(X_train, y_train_ec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ec = clf_ec.predict(X_test)\n",
    "print(classification_report(y_test_ec, y_pred_ec.tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
